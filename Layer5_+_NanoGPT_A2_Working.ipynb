{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfo7cT8nZfie"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7H6Nr2kZocU"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca5nKlSqZk5V",
        "outputId": "372f0843-5b99-4c7c-b36d-88e04ff90a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TinyStories-10k generated. Lines: 10000\n",
            "Merges: 200\n",
            "Final vocab size: 275\n",
            "Tokenizer saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------\n",
        "# TinyStories-10k + BPE Tokenizer (All-in-One Cell)\n",
        "# -------------------------------------------------\n",
        "\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# STEP 1 — Generate TinyStories-10k IN COLAB\n",
        "# ===========================================\n",
        "\n",
        "def generate_story():\n",
        "    subjects = [\n",
        "        \"a little dragon\", \"a young wizard\", \"a brave robot\", \"a curious child\",\n",
        "        \"a tiny bear\", \"a happy fairy\", \"a small puppy\", \"a gentle giant\",\n",
        "        \"a clever cat\", \"a playful elf\"\n",
        "    ]\n",
        "\n",
        "    actions = [\n",
        "        \"found\", \"lost\", \"built\", \"discovered\", \"met\", \"followed\", \"searched for\",\n",
        "        \"protected\", \"explored\", \"dreamed about\"\n",
        "    ]\n",
        "\n",
        "    objects = [\n",
        "        \"a magic book\", \"a glowing key\", \"a secret door\", \"a hidden map\",\n",
        "        \"a tiny spaceship\", \"an ancient scroll\", \"a shiny crystal\",\n",
        "        \"a mysterious potion\", \"a floating island\", \"a talking tree\"\n",
        "    ]\n",
        "\n",
        "    endings = [\n",
        "        \"and learned something new.\",\n",
        "        \"and became very brave.\",\n",
        "        \"and made a new friend.\",\n",
        "        \"and felt very happy.\",\n",
        "        \"and went on an adventure.\",\n",
        "        \"and found the meaning of courage.\",\n",
        "        \"and discovered a new world.\",\n",
        "        \"and shared the story with everyone.\",\n",
        "        \"which changed their life forever.\",\n",
        "        \"but the journey had just begun.\"\n",
        "    ]\n",
        "\n",
        "    story = f\"Once upon a time, {random.choice(subjects)} {random.choice(actions)} {random.choice(objects)} {random.choice(endings)}\"\n",
        "    return story\n",
        "\n",
        "\n",
        "# Generate 10,000 synthetic TinyStories lines\n",
        "stories = [generate_story() for _ in range(10000)]\n",
        "\n",
        "with open(\"tinystories_10k.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for s in stories:\n",
        "        f.write(s + \"\\n\")\n",
        "\n",
        "print(\"TinyStories-10k generated. Lines:\", len(stories))\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# STEP 2 — BPE Tokenizer\n",
        "# ===========================================\n",
        "\n",
        "# -------------------------------------------------\n",
        "# tokenizer.py — FIXED and stable BPE Tokenizer\n",
        "# -------------------------------------------------\n",
        "\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, vocab_size=5000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_freqs = Counter()\n",
        "        self.bpe_merges = []\n",
        "        self.vocab = {}\n",
        "        self.inv_vocab = {}\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # Train BPE tokenizer\n",
        "    # -----------------------------------------------------\n",
        "    def train(self, text):\n",
        "        words = text.split()\n",
        "\n",
        "        # Count words in character form\n",
        "        for w in words:\n",
        "            tokens = list(w) + [\"</w>\"]\n",
        "            self.word_freqs[tuple(tokens)] += 1\n",
        "\n",
        "        # Perform merges until we reach vocab_size\n",
        "        while len(self.bpe_merges) < (self.vocab_size - 256):   # leaving room for chars\n",
        "            pairs = self._get_pair_counts()\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            self.bpe_merges.append(best_pair)\n",
        "            self._merge_pair(best_pair)\n",
        "\n",
        "            if len(self.bpe_merges) % 200 == 0:\n",
        "                print(\"Merges:\", len(self.bpe_merges))\n",
        "\n",
        "        # Build final vocab from:\n",
        "        # - all characters seen\n",
        "        # - all merged pairs\n",
        "        vocab = set()\n",
        "\n",
        "        # characters\n",
        "        for word in self.word_freqs:\n",
        "            for tok in word:\n",
        "                vocab.add(tok)\n",
        "\n",
        "        # merged tokens\n",
        "        for a, b in self.bpe_merges:\n",
        "            vocab.add(a + b)\n",
        "\n",
        "        vocab.add(\"</w>\")  # ensure always present\n",
        "\n",
        "        vocab = sorted(list(vocab))\n",
        "        self.vocab = {tok: i for i, tok in enumerate(vocab)}\n",
        "        self.inv_vocab = {i: tok for tok, i in self.vocab.items()}\n",
        "\n",
        "        print(\"Final vocab size:\", len(self.vocab))\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def _get_pair_counts(self):\n",
        "        pairs = Counter()\n",
        "        for word, freq in self.word_freqs.items():\n",
        "            syms = list(word)\n",
        "            for i in range(len(syms) - 1):\n",
        "                pairs[(syms[i], syms[i+1])] += freq\n",
        "        return pairs\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def _merge_pair(self, pair):\n",
        "        new_freqs = Counter()\n",
        "        bigram = \" \".join(pair)\n",
        "        pat = re.compile(re.escape(bigram))\n",
        "\n",
        "        for word, freq in self.word_freqs.items():\n",
        "            w = \" \".join(word)\n",
        "            w_new = pat.sub(\"\".join(pair), w)\n",
        "            new_freqs[tuple(w_new.split())] += freq\n",
        "\n",
        "        self.word_freqs = new_freqs\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def _apply_bpe(self, tokens):\n",
        "        merges_set = set(tuple(m) for m in self.bpe_merges)\n",
        "        changed = True\n",
        "\n",
        "        while changed:\n",
        "            changed = False\n",
        "            i = 0\n",
        "            while i < len(tokens)-1:\n",
        "                if (tokens[i], tokens[i+1]) in merges_set:\n",
        "                    tokens = tokens[:i] + [tokens[i] + tokens[i+1]] + tokens[i+2:]\n",
        "                    changed = True\n",
        "                else:\n",
        "                    i += 1\n",
        "        return tokens\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def encode(self, text):\n",
        "        ids = []\n",
        "        for w in text.split():\n",
        "            tokens = self._apply_bpe(list(w) + [\"</w>\"])\n",
        "            for t in tokens:\n",
        "                ids.append(self.vocab.get(t, self.vocab[\"</w>\"]))\n",
        "        return ids\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def decode(self, ids):\n",
        "        toks = [self.inv_vocab[i] for i in ids]\n",
        "        text = \"\".join(toks)\n",
        "        return text.replace(\"</w>\", \" \").strip()\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    def save(self, folder=\"bpe_tokenizer\"):\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        json.dump(self.vocab, open(f\"{folder}/vocab.json\", \"w\"))\n",
        "        json.dump(self.bpe_merges, open(f\"{folder}/merges.json\", \"w\"))\n",
        "\n",
        "    @staticmethod\n",
        "    def load(folder=\"bpe_tokenizer\"):\n",
        "        tok = BPETokenizer()\n",
        "        tok.vocab = json.load(open(f\"{folder}/vocab.json\"))\n",
        "        tok.inv_vocab = {v: k for k, v in tok.vocab.items()}\n",
        "        tok.bpe_merges = json.load(open(f\"{folder}/merges.json\"))\n",
        "        return tok\n",
        "\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# STEP 3 — Train and save tokenizer\n",
        "# ===========================================\n",
        "\n",
        "text = open(\"tinystories_10k.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "tokenizer = BPETokenizer(vocab_size=5000)\n",
        "tokenizer.train(text)\n",
        "tokenizer.save(\"bpe_tokenizer\")\n",
        "\n",
        "print(\"Tokenizer saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjndr0XfZ2jk"
      },
      "source": [
        "Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ROxwXDiZk8N",
        "outputId": "3f0659ee-4dfc-4f4c-f0d5-baa12f8607cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset chars: 852012\n",
            "Loaded tokenizer, vocab size: 275\n",
            "Total tokens: 230926\n",
            "Train tokens: 207833, Val tokens: 23093\n",
            "Saved train/val tensors to 'bin/'\n",
            "Sample token ids (first 60): [4, 254, 5, 247, 5, 34, 1, 207, 152, 231, 1, 5, 238, 252, 41, 240, 1, 138, 121, 139, 231, 1, 28, 4, 254, 5, 247, 5, 273, 266]\n",
            "Decoded sample: Once upon a time, a brave robot lost a talking tree but the journey had just begun. Once upon a time, a young wizard discovered an ancient  cr ll and  hared the story with everyone. Once upon a time, \n",
            "Dataset prepared — ready to train. Call get_batch('train') to get batches.\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------\n",
        "# GoLab Cell 2 — Dataset Preparation\n",
        "# ------------------------------\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# parameters (tweak if you want)\n",
        "batch_size = 64\n",
        "block_size = 256   # context length for model\n",
        "val_ratio = 0.1\n",
        "seed = 1337\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# ---- 1) Paths ----\n",
        "dataset_path = \"tinystories_10k.txt\"\n",
        "tokenizer_folder = \"bpe_tokenizer\"\n",
        "save_dir = \"bin\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ---- 2) Load raw text ----\n",
        "assert Path(dataset_path).exists(), f\"Dataset not found: {dataset_path}\"\n",
        "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "print(\"Loaded dataset chars:\", len(text))\n",
        "\n",
        "# ---- 3) Load tokenizer ----\n",
        "# We assume BPETokenizer class is defined in the notebook (tokenizer cell).\n",
        "tokenizer = BPETokenizer.load(tokenizer_folder)\n",
        "print(\"Loaded tokenizer, vocab size:\", len(tokenizer.vocab))\n",
        "\n",
        "# ---- 4) Encode entire corpus to integer IDs ----\n",
        "# Note: tokenizer.encode works on strings (splits on whitespace internally)\n",
        "all_ids = tokenizer.encode(text)\n",
        "data = torch.tensor(all_ids, dtype=torch.long)\n",
        "print(\"Total tokens:\", data.size(0))\n",
        "\n",
        "# ---- 5) Train / Val split ----\n",
        "n = int((1 - val_ratio) * len(data))\n",
        "train_data = data[:n].clone()\n",
        "val_data = data[n:].clone()\n",
        "print(f\"Train tokens: {train_data.size(0)}, Val tokens: {val_data.size(0)}\")\n",
        "\n",
        "# ---- 6) Save tensors to disk ----\n",
        "torch.save(train_data, os.path.join(save_dir, \"train.pt\"))\n",
        "torch.save(val_data, os.path.join(save_dir, \"val.pt\"))\n",
        "print(\"Saved train/val tensors to 'bin/'\")\n",
        "\n",
        "# ---- 7) Basic get_batch function (returns x,y on device) ----\n",
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      x, y: LongTensors of shape (batch_size, block_size)\n",
        "      x = input tokens, y = target tokens (shifted by one)\n",
        "    \"\"\"\n",
        "    data_src = train_data if split == 'train' else val_data\n",
        "    # pick random starting indices\n",
        "    ix = torch.randint(0, len(data_src) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data_src[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_src[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# ---- 8) Sanity checks (decode a small slice) ----\n",
        "sample_index = 0\n",
        "sample_slice = train_data[sample_index: sample_index + 60].tolist()\n",
        "print(\"Sample token ids (first 60):\", sample_slice[:30])\n",
        "try:\n",
        "    decoded = tokenizer.decode(sample_slice)\n",
        "    print(\"Decoded sample:\", decoded[:200])\n",
        "except Exception as e:\n",
        "    print(\"Decode failed (ok for some BPE id layouts):\", e)\n",
        "\n",
        "# ---- done ----\n",
        "print(\"Dataset prepared — ready to train. Call get_batch('train') to get batches.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYbz5RD6Z87B"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux_jcsisZk_O",
        "outputId": "60145e54-47fe-498d-b25b-78c23435fda5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits shape: torch.Size([2, 32, 1000]) loss: 6.535799503326416\n",
            "Params (M): 0.928\n"
          ]
        }
      ],
      "source": [
        "# models/model.py\n",
        "# NanoGPT-A2 — minimal GPT-style model (PyTorch)\n",
        "# Clean, readable, research-friendly, and ready for Layer-5 hooks.\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class GPTConfig:\n",
        "    \"\"\"Minimal config container.\"\"\"\n",
        "    def __init__(self, vocab_size, block_size,\n",
        "                 n_layer=6, n_head=6, n_embd=384, dropout=0.2):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "\n",
        "# -------------------------\n",
        "# Attention head (single)\n",
        "# -------------------------\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, config, head_size):\n",
        "        super().__init__()\n",
        "        n_embd = config.n_embd\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # causal mask: registered as buffer so it moves with model.to(device)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C)\n",
        "        B, T, C = x.size()\n",
        "        k = self.key(x)    # (B, T, hs)\n",
        "        q = self.query(x)  # (B, T, hs)\n",
        "        # compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * (k.size(-1) ** -0.5)  # (B, T, T)\n",
        "        # causal masking (prevent attending to future)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)  # (B, T, hs)\n",
        "        out = wei @ v      # (B, T, hs)\n",
        "        return out\n",
        "\n",
        "# -------------------------\n",
        "# Multi-head attention\n",
        "# -------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        head_size = config.n_embd // config.n_head\n",
        "        self.heads = nn.ModuleList([Head(config, head_size) for _ in range(config.n_head)])\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B, T, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "# -------------------------\n",
        "# Feed-forward network (MLP)\n",
        "# -------------------------\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# -------------------------\n",
        "# Transformer block\n",
        "# -------------------------\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --- attention (with residual) ---\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        # --- MLP (with residual) ---\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# -------------------------\n",
        "# Full GPT language model\n",
        "# -------------------------\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # token and positional embeddings\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Embedding(config.block_size, config.n_embd)\n",
        "\n",
        "        # stack of transformer blocks\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)  # final layer norm\n",
        "\n",
        "        # language modeling head (tie weights optionally)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # tie weights like GPT-2: lm_head weight = tok_emb weight\n",
        "        self.lm_head.weight = self.tok_emb.weight\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "        if isinstance(module, nn.LayerNorm):\n",
        "            nn.init.zeros_(module.bias)\n",
        "            nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        idx: (B, T) token indices\n",
        "        targets: (B, T) token indices (optional)\n",
        "        returns: logits (B, T, V), loss (scalar) if targets provided\n",
        "        \"\"\"\n",
        "        device = idx.device\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Sequence length {T} > block_size {self.config.block_size}\"\n",
        "\n",
        "        # token + position embeddings\n",
        "        tok_emb = self.tok_emb(idx)                       # (B, T, C)\n",
        "        pos = torch.arange(T, device=device)\n",
        "        pos_emb = self.pos_emb(pos)                       # (T, C)\n",
        "        x = tok_emb + pos_emb                              # (B, T, C)\n",
        "\n",
        "        # --- Optionally: capture residual stream after embeddings for Layer-5 hooks ---\n",
        "        # e.g., residual = x.clone() or call registered hooks here\n",
        "\n",
        "        # transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln_f(x)                                   # (B, T, C)\n",
        "        logits = self.lm_head(x)                           # (B, T, V)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # reshape for cross entropy\n",
        "            loss = F.cross_entropy(logits.view(B*T, -1), targets.view(B*T))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Auto-regressive generation.\n",
        "        idx: (B, T) starting context\n",
        "        returns: (B, T + max_new_tokens)\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.config.block_size:]  # crop to block_size\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature     # (B, V)\n",
        "\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                min_topk = v[:, -1].unsqueeze(1)\n",
        "                logits = torch.where(logits < min_topk, torch.full_like(logits, -1e10), logits)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)            # (B, V)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            idx = torch.cat([idx, next_token], dim=1)             # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "# Utility: count parameters\n",
        "# -------------------------\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())/1e6\n",
        "\n",
        "# -------------------------\n",
        "# Quick smoke test when run as script\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # small test to validate shapes\n",
        "    cfg = GPTConfig(vocab_size=1000, block_size=64, n_layer=4, n_head=4, n_embd=128, dropout=0.1)\n",
        "    m = GPTLanguageModel(cfg)\n",
        "    x = torch.randint(0, cfg.vocab_size, (2, 32))\n",
        "    logits, loss = m(x, targets=x)\n",
        "    print(\"logits shape:\", logits.shape, \"loss:\", loss.item())\n",
        "    print(\"Params (M):\", count_parameters(m))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Io082sNaDHT"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN78E-DVZlB4",
        "outputId": "4af692a2-5301-477b-b7e4-1a8f9bb29aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 275\n",
            "Train tokens: 207833\n",
            "Val tokens: 23093\n",
            "Model parameters: 10.84M\n",
            "Step 0: train loss 5.7343, val loss 5.7345\n",
            "Checkpoint saved.\n",
            "Step 200: train loss 0.5002, val loss 0.5010\n",
            "Checkpoint saved.\n",
            "Step 400: train loss 0.4181, val loss 0.4199\n",
            "Checkpoint saved.\n",
            "Step 600: train loss 0.4067, val loss 0.4094\n",
            "Checkpoint saved.\n",
            "Step 800: train loss 0.4062, val loss 0.4085\n",
            "Checkpoint saved.\n",
            "Step 1000: train loss 0.4033, val loss 0.4070\n",
            "Checkpoint saved.\n",
            "Step 1200: train loss 0.4086, val loss 0.4123\n",
            "Checkpoint saved.\n",
            "Step 1400: train loss 0.4030, val loss 0.4071\n",
            "Checkpoint saved.\n",
            "Step 1499: train loss 0.4020, val loss 0.4061\n",
            "Checkpoint saved.\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# ---------- TRAIN.PY (COLAB VERSION) ----------\n",
        "# Assumes:\n",
        "# - BPETokenizer class is already defined in notebook\n",
        "# - GPTConfig and GPTLanguageModel are already defined in notebook\n",
        "# - train.pt / val.pt created\n",
        "\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# ---------------------\n",
        "# Hyperparameters\n",
        "# ---------------------\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "learning_rate = 3e-4\n",
        "max_iters = 1500\n",
        "eval_interval = 200\n",
        "eval_iters = 100\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# ---------------------\n",
        "# Load dataset\n",
        "# ---------------------\n",
        "train_data = torch.load(\"bin/train.pt\")\n",
        "val_data   = torch.load(\"bin/val.pt\")\n",
        "\n",
        "# ---------------------\n",
        "# Load tokenizer\n",
        "# ---------------------\n",
        "tokenizer = BPETokenizer.load(\"bpe_tokenizer\")\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Train tokens:\", len(train_data))\n",
        "print(\"Val tokens:\", len(val_data))\n",
        "\n",
        "# ---------------------\n",
        "# get_batch function\n",
        "# ---------------------\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# ---------------------\n",
        "# Evaluation\n",
        "# ---------------------\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    model.eval()\n",
        "    out = {}\n",
        "\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# ---------------------\n",
        "# Build model\n",
        "# ---------------------\n",
        "config = GPTConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=block_size,\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.2,\n",
        ")\n",
        "\n",
        "model = GPTLanguageModel(config).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "# ---------------------\n",
        "# Training loop\n",
        "# ---------------------\n",
        "for it in range(max_iters):\n",
        "\n",
        "    if it % eval_interval == 0 or it == max_iters - 1:\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"Step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # save checkpoint\n",
        "        torch.save(\n",
        "            {\n",
        "                'model': model.state_dict(),\n",
        "                'config': config.__dict__,\n",
        "            },\n",
        "            \"checkpoint.pt\"\n",
        "        )\n",
        "        print(\"Checkpoint saved.\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sc26u-La57Z"
      },
      "source": [
        "Baseline Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpg1u16aZlEA",
        "outputId": "3ddd4a30-f370-4fb4-b1f8-ab6a8370ef47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Tokenizer loaded. Vocab size: 275\n",
            "Train tokens: 207833\n",
            "Val tokens  : 23093\n",
            "✓ Model restored from checkpoint.\n",
            "\n",
            "========== Computing Perplexities ==========\n",
            "Train Loss: 0.40178808391094206 Train PPL: 1.4944945916669858\n",
            "Val Loss : 0.4065910530090332 Val PPL : 1.5016898685221982\n",
            "\n",
            "========== Collecting Layer Stats ==========\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:58<00:00,  1.18s/it]\n",
            "100%|██████████| 50/50 [00:58<00:00,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========== Computing Drift ==========\n",
            "\n",
            "✓ Evaluation complete. Results saved to eval_baseline.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CLEAN BASELINE EVALUATION FOR NANOGPT-A2 (COLAB VERSION)\n",
        "# ============================================================\n",
        "\n",
        "import os, json, math, torch\n",
        "from tqdm import trange\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1. TOKENIZER (already defined earlier in Colab)\n",
        "# ============================================================\n",
        "if not os.path.exists(\"bpe_tokenizer/vocab.json\"):\n",
        "    raise FileNotFoundError(\"Tokenizer not found.\")\n",
        "\n",
        "tokenizer = BPETokenizer.load(\"bpe_tokenizer\")\n",
        "print(\"Tokenizer loaded. Vocab size:\", len(tokenizer.vocab))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. LOAD TRAIN + VAL DATA\n",
        "# ============================================================\n",
        "train_data = torch.load(\"bin/train.pt\")\n",
        "val_data   = torch.load(\"bin/val.pt\")\n",
        "print(\"Train tokens:\", len(train_data))\n",
        "print(\"Val tokens  :\", len(val_data))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. BUILD MODEL (already defined earlier in Colab)\n",
        "# ============================================================\n",
        "config = GPTConfig(\n",
        "    vocab_size=len(tokenizer.vocab),\n",
        "    block_size=256,\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.2\n",
        ")\n",
        "\n",
        "model = GPTLanguageModel(config).to(device)\n",
        "\n",
        "if not os.path.exists(\"checkpoint.pt\"):\n",
        "    raise FileNotFoundError(\"checkpoint.pt missing.\")\n",
        "\n",
        "ckpt = torch.load(\"checkpoint.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "print(\"✓ Model restored from checkpoint.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. GET BATCH + PERPLEXITY\n",
        "# ============================================================\n",
        "def get_batch(data_tensor, batch_size=32, block_size=256):\n",
        "    ix = torch.randint(0, len(data_tensor) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data_tensor[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_tensor[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_perplexity(model, data_tensor, n_iter=50, batch_size=32):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for _ in range(n_iter):\n",
        "        xb, yb = get_batch(data_tensor, batch_size, config.block_size)\n",
        "        _, loss = model(xb, yb)\n",
        "        losses.append(loss.item())\n",
        "    model.train()\n",
        "    loss = sum(losses) / len(losses)\n",
        "    return loss, math.exp(loss)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. STREAMING STATS PER LAYER\n",
        "# ============================================================\n",
        "class RunningStats:\n",
        "    def __init__(self, C):\n",
        "        self.count = 0\n",
        "        self.mean = torch.zeros(C)\n",
        "        self.M2 = torch.zeros(C)\n",
        "\n",
        "    def update(self, x):\n",
        "        x = x.reshape(-1, x.size(-1)).detach().cpu()\n",
        "        for row in x:\n",
        "            self.count += 1\n",
        "            delta = row - self.mean\n",
        "            self.mean += delta / self.count\n",
        "            delta2 = row - self.mean\n",
        "            self.M2 += delta * delta2\n",
        "\n",
        "    def finalize(self):\n",
        "        var = self.M2 / max(1, self.count - 1)\n",
        "        return {\n",
        "            \"mean\": self.mean.tolist(),\n",
        "            \"std\": torch.sqrt(var).tolist(),\n",
        "            \"mean_norm\": float(self.mean.norm().item())\n",
        "        }\n",
        "\n",
        "\n",
        "def collect_streaming_stats(model, data_tensor, n_batches=50, batch_size=32):\n",
        "    C = model.config.n_embd\n",
        "    stats = {i: RunningStats(C) for i in range(model.config.n_layer)}\n",
        "\n",
        "    hooks = []\n",
        "    def hook_factory(layer):\n",
        "        def hook(module, inp, out):\n",
        "            stats[layer].update(out)\n",
        "        return hook\n",
        "\n",
        "    for i, block in enumerate(model.blocks):\n",
        "        hooks.append(block.register_forward_hook(hook_factory(i)))\n",
        "\n",
        "    for _ in trange(n_batches):\n",
        "        xb, _ = get_batch(data_tensor, batch_size, config.block_size)\n",
        "        model(xb)\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    return {i: stats[i].finalize() for i in stats}\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6. JSON-SAFE CONVERSION\n",
        "# ============================================================\n",
        "def to_python(obj):\n",
        "    \"\"\"Recursively convert tensors → lists/floats for JSON.\"\"\"\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        if obj.dim() == 0:\n",
        "            return obj.item()\n",
        "        return obj.tolist()\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: to_python(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, list):\n",
        "        return [to_python(x) for x in obj]\n",
        "    return obj\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7. MAIN EVAL\n",
        "# ============================================================\n",
        "print(\"\\n========== Computing Perplexities ==========\")\n",
        "train_loss, train_ppl = compute_perplexity(model, train_data)\n",
        "val_loss, val_ppl = compute_perplexity(model, val_data)\n",
        "\n",
        "print(\"Train Loss:\", train_loss, \"Train PPL:\", train_ppl)\n",
        "print(\"Val Loss :\", val_loss, \"Val PPL :\", val_ppl)\n",
        "\n",
        "print(\"\\n========== Collecting Layer Stats ==========\")\n",
        "stats_train = collect_streaming_stats(model, train_data)\n",
        "stats_val   = collect_streaming_stats(model, val_data)\n",
        "\n",
        "print(\"\\n========== Computing Drift ==========\")\n",
        "drift = {}\n",
        "for layer in stats_train:\n",
        "    m1 = torch.tensor(stats_train[layer][\"mean\"])\n",
        "    m2 = torch.tensor(stats_val[layer][\"mean\"])\n",
        "    l2 = float(torch.norm(m1 - m2))\n",
        "    rel = l2 / (torch.norm(m1) + 1e-12)\n",
        "    drift[layer] = {\"l2\": l2, \"relative\": rel}\n",
        "\n",
        "results = {\n",
        "    \"train_loss\": train_loss, \"train_ppl\": train_ppl,\n",
        "    \"val_loss\": val_loss, \"val_ppl\": val_ppl,\n",
        "    \"stats_train\": stats_train,\n",
        "    \"stats_val\": stats_val,\n",
        "    \"activation_drift\": drift\n",
        "}\n",
        "\n",
        "# JSON SAFE OUTPUT\n",
        "results = to_python(results)\n",
        "\n",
        "json.dump(results, open(\"eval_baseline.json\", \"w\"), indent=2)\n",
        "print(\"\\n✓ Evaluation complete. Results saved to eval_baseline.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LVRjvuJbD0z"
      },
      "source": [
        "Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91N_2YhbZlGa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"eval_baseline.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "import pprint\n",
        "#pprint.pprint(data)\n",
        "print(\"Train Loss:\", data[\"train_loss\"])\n",
        "print(\"Train Perplexity:\", data[\"train_ppl\"])\n",
        "print(\"Val Loss:\", data[\"val_loss\"])\n",
        "print(\"Val Perplexity:\", data[\"val_ppl\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onfz9L2fbQ97"
      },
      "source": [
        "Basline Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0dRJANdZlI2"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------\n",
        "# Baseline Metrics Viewer\n",
        "# ----------------------------------\n",
        "\n",
        "import json, torch\n",
        "from pprint import pprint\n",
        "\n",
        "with open(\"eval_baseline.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"========== BASIC METRICS ==========\")\n",
        "print(\"Train Loss:\", data[\"train_loss\"])\n",
        "print(\"Train PPL :\", data[\"train_ppl\"])\n",
        "print(\"Val Loss  :\", data[\"val_loss\"])\n",
        "print(\"Val PPL   :\", data[\"val_ppl\"])\n",
        "print(\"\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 1. Residual Activation Mean Norms per Layer\n",
        "# -------------------------------------------------------\n",
        "print(\"========== RESIDUAL MEAN NORM (Train) ==========\")\n",
        "for layer, stats in data[\"residual_stats_train\"].items():\n",
        "    print(f\"Layer {layer}: mean_norm = {stats['mean_norm']:.4f}\")\n",
        "\n",
        "print(\"\\n========== RESIDUAL MEAN NORM (Val) ==========\")\n",
        "for layer, stats in data[\"residual_stats_val\"].items():\n",
        "    print(f\"Layer {layer}: mean_norm = {stats['mean_norm']:.4f}\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 2. Residual Activation STD Norm per Layer\n",
        "# -------------------------------------------------------\n",
        "print(\"\\n========== RESIDUAL STD NORM (Train) ==========\")\n",
        "for layer, stats in data[\"residual_stats_train\"].items():\n",
        "    std_vec = torch.tensor(stats[\"std\"])\n",
        "    print(f\"Layer {layer}: std_norm = {std_vec.norm().item():.4f}\")\n",
        "\n",
        "print(\"\\n========== RESIDUAL STD NORM (Val) ==========\")\n",
        "for layer, stats in data[\"residual_stats_val\"].items():\n",
        "    std_vec = torch.tensor(stats[\"std\"])\n",
        "    print(f\"Layer {layer}: std_norm = {std_vec.norm().item():.4f}\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 3. Activation Drift\n",
        "# -------------------------------------------------------\n",
        "print(\"\\n========== ACTIVATION DRIFT (Train ↔ Val) ==========\")\n",
        "for layer, drift_vals in data[\"activation_drift\"].items():\n",
        "    print(f\"Layer {layer}: L2={drift_vals['l2']:.4f}, Relative={drift_vals['relative']:.4f}\")\n",
        "\n",
        "print(\"\\nDone viewing all baseline metrics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhTkZnA9ZlMc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}